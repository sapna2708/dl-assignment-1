{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uq11BlBg2P1k",
    "outputId": "bd3e893d-1eb9-4ea8-eebe-4ded80fc0604",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (0.16.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (3.1.42)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (1.40.6)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (4.23.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBQq4MIPL10I",
    "outputId": "feccec2b-b4da-4d1c-dd02-edd97e19819b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wandb' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wandb login 5c9ed409dd1eb8a3a2037c9944c0bbb74bd551b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "44erUjY6te7_",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "import math\n",
    "import time\n",
    "import wandb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGMOID_KEY = \"sigmoid\"\n",
    "TANH_KEY = \"tanh\"\n",
    "RELU_KEY = \"ReLU\"\n",
    "XAVIER_KEY = \"Xavier\"\n",
    "RANDOM_KEY = \"random\"\n",
    "HE_KEY = \"HE\"\n",
    "SGD_KEY=\"sgd\"\n",
    "MGD_KEY=\"momentum\"\n",
    "NAG_KEY=\"nag\"\n",
    "RMSPROP_KEY=\"rmsprop\"\n",
    "ADAM_KEY=\"adam\"\n",
    "NADAM_KEY=\"nadam\"\n",
    "\n",
    "CROSS_ENTROPY_KEY = 'cross_entropy'\n",
    "MEAN_SQUARE_KEY = 'mean_squared_error'\n",
    "\n",
    "FASHION_MNIST_DATASET_KEY = 'fashion_mnist'\n",
    "MNIST_DATASET_KEY = 'mnist'\n",
    "\n",
    "GRAD_A = \"del_a\"\n",
    "GRAD_W = \"del_w\"\n",
    "GRAD_H = \"del_h\"\n",
    "GRAD_B = \"del_b\"\n",
    "\n",
    "PROJECT_NAME_KEY = \"dl-assignment-1\"\n",
    "PROJECT_ENTITY = \"dl-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzyhH7e-wW06",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Activation Functions and it's derivative\n",
    "\n",
    "- Mean Square\n",
    "- Sigmoid\n",
    "- Soft Max\n",
    "- Cross Entropy\n",
    "- tanh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xl4qhtI4vdm9"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # z = np.clip(z,500,-500)\n",
    "    return 1.0 / (1 + np.exp(-(z)))\n",
    "\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "\n",
    "def sin(z):\n",
    "    return np.sin(z)\n",
    "\n",
    "\n",
    "def reLu(z):\n",
    "    return (z>0)*(z) + ((z<0)*(z)*0.01)\n",
    "    #return np.maximum(z,0)\n",
    "    #return np.where(z<0, 0.01*z, z)\n",
    "\n",
    "def softmax(Z):\n",
    "    Z -= np.max(Z)\n",
    "    exp_Z = np.exp(Z)\n",
    "    softmax_output = exp_Z / np.sum(exp_Z)\n",
    "    return softmax_output\n",
    "\n",
    "\n",
    "def del_sigmoid(z):\n",
    "    # z = np.clip(z,500,-500)\n",
    "    return  (1.0 / (1 + np.exp(-(z))))*(1 -  1.0 / (1 + np.exp(-(z))))\n",
    "\n",
    "def del_tanh(z):\n",
    "    return 1 - np.tanh(z) ** 2\n",
    "\n",
    "\n",
    "def del_reLu(z):\n",
    "    return (z>0)*np.ones(z.shape) + (z<0)*(0.01*np.ones(z.shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1jrcToSKIPY",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Utility Functions\n",
    "\n",
    "Initializers\n",
    "- Xavier\n",
    "- Random\n",
    "- He\n",
    "\n",
    "Loss Function\n",
    "- Cross Entropy\n",
    "- Mean Square Error\n",
    "\n",
    "oneHotEncode\n",
    "\n",
    "accuracy\n",
    "\n",
    "printAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qw3YPXh2KHZW"
   },
   "outputs": [],
   "source": [
    "def Xavier_initializer(dim):\n",
    "    '''\n",
    "    Xavier weight initialization for neural networks.\n",
    "\n",
    "    Parameters:\n",
    "    - dim: Tuple (output_dim, input_dim) representing the dimensions of the weight matrix.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of shape (output_dim, input_dim) with Xavier-initialized values.\n",
    "    '''\n",
    "    xavier_stddev = np.sqrt(2 / (dim[1] + dim[0]))\n",
    "    return np.random.normal(0, xavier_stddev, size=(dim[0], dim[1]))\n",
    "\n",
    "def random_initializer(dim):\n",
    "    '''\n",
    "    Random weight initialization for neural networks.\n",
    "\n",
    "    Parameters:\n",
    "    - dim: Tuple (output_dim, input_dim) representing the dimensions of the weight matrix.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of shape (output_dim, input_dim) with randomly initialized values.\n",
    "    '''\n",
    "    return np.random.normal(0, 1, size=(dim[0], dim[1]))\n",
    "\n",
    "def He_initializer(dim):\n",
    "    '''\n",
    "    He weight initialization for neural networks.\n",
    "\n",
    "    Parameters:\n",
    "    - dim: Tuple (output_dim, input_dim) representing the dimensions of the weight matrix.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of shape (output_dim, input_dim) with He-initialized values.\n",
    "    '''\n",
    "    He_stddev = np.sqrt(2 / (dim[1]))\n",
    "    return np.random.normal(0, 1, size=(dim[0], dim[1])) * He_stddev\n",
    "\n",
    "def meanSquaredErrorLoss(Y_true, Y_pred):\n",
    "    '''\n",
    "    Calculates the Mean Squared Error (MSE) loss between true and predicted values.\n",
    "\n",
    "    Arguments:\n",
    "    - Y_true (numpy.ndarray): True output labels.\n",
    "    - Y_pred (numpy.ndarray): Predicted output labels.\n",
    "\n",
    "    Returns:\n",
    "    - float: Mean Squared Error loss.\n",
    "    '''\n",
    "    return np.mean((Y_true - Y_pred) * (Y_true - Y_pred))\n",
    "\n",
    "def crossEntropyLoss( Y_true, Y_pred):\n",
    "    '''\n",
    "    Calculates the Cross-Entropy loss between true and predicted probability distributions.\n",
    "\n",
    "    Arguments:\n",
    "    - Y_true (numpy.ndarray): True output labels in one-hot encoded form.\n",
    "    - Y_pred (numpy.ndarray): Predicted probability distributions.\n",
    "\n",
    "    Returns:\n",
    "    - float: Cross-Entropy loss.\n",
    "    '''\n",
    "    eps = 1e-15\n",
    "    Y_pred = np.clip(Y_pred,eps,1.0-eps)\n",
    "    loss = -np.sum(Y_true*np.log(Y_pred),axis=1)\n",
    "    loss = np.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def oneHotEncoding(num_classes, train_raw):\n",
    "    '''\n",
    "    Performs one-hot encoding on the provided labels.\n",
    "\n",
    "    Parameters:\n",
    "    - Y_train_raw (numpy.ndarray): Raw output labels.\n",
    "\n",
    "    Returns:\n",
    "    - Ydata (numpy.ndarray): One-hot encoded representation of the labels.\n",
    "    '''\n",
    "    return np.array([[1.0 if int(train_raw[i]) == j else 0.0 for i in range(train_raw.shape[0])] for j in range(num_classes)])\n",
    "\n",
    "def printAccuracy(epoch, training_loss, training_acc, validation_acc, elapsed, alpha):\n",
    "    print(f\"Epoch: {epoch}, \"\n",
    "          f\"Loss: {training_loss:.3e}, \"\n",
    "          f\"Training Accuracy: {training_acc:.2f}, \"\n",
    "          f\"Validation Accuracy: {validation_acc:.2f}, \"\n",
    "          f\"Time: {elapsed:.2f}s, \"\n",
    "          f\"Learning Rate: {alpha:.3e}\")\n",
    "\n",
    "\n",
    "def accuracy(Y_true, Y_pred, data_size):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the model's predictions.\n",
    "\n",
    "    Arguments:\n",
    "    - Y_true (numpy.ndarray): True output labels in one-hot encoded form.\n",
    "    - Y_pred (numpy.ndarray): Predicted output labels in one-hot encoded form.\n",
    "    - data_size (int): Number of samples in the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - float: Accuracy of the model.\n",
    "    - list: True labels.\n",
    "    - list: Predicted labels.\n",
    "    \"\"\"\n",
    "    Y_true_vals = np.argmax(Y_true, axis=0).tolist()\n",
    "    Y_pred_vals = np.argmax(Y_pred, axis=0).tolist()\n",
    "    \n",
    "    correct_vals = sum(yt == yp for yt, yp in zip(Y_true_vals, Y_pred_vals))\n",
    "    acc = correct_vals / data_size\n",
    "    \n",
    "    return acc, Y_true_vals, Y_pred_vals\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4x84zeAtIR3",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Question-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518,
     "referenced_widgets": [
      "23a94a82a6ca42ec946fcf3907061507",
      "4ea330b8112144aaa609f227c69ab43d",
      "8203cd648034444392447138fd311a3b",
      "5eafd61b8548431a97329158b4e5e769",
      "7ccc92268f6e4ed68b4e4d04209d61e3",
      "df3cd51143304151bcf7424531ceb7e0",
      "6353176c740345eba3b2b15badc59fba",
      "436d84ba036341b5b43468382725e5c4"
     ]
    },
    "id": "3FkX3-9yvmd6",
    "outputId": "1fa4628f-c149-45f4-a61e-e2a702e75a5c"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "(trainIn, trainOut), (testIn, testOut) = fashion_mnist.load_data()\n",
    "\n",
    "# Define class labels\n",
    "class_labels = [\n",
    "    \"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"\n",
    "]\n",
    "\n",
    "# Initialize figure\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "fig.suptitle(\"Fashion MNIST Classes\", fontsize=16)\n",
    "\n",
    "image_list, image_labels = [], []\n",
    "\n",
    "# Display one sample per class\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    class_idx = np.where(trainOut == i)[0][0]  # Get first occurrence of each class\n",
    "    ax.imshow(trainIn[class_idx], cmap=\"gray\")\n",
    "    ax.set_title(class_labels[i], fontsize=12)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Store images and labels for logging in WandB\n",
    "    image_list.append(trainIn[class_idx])\n",
    "    image_labels.append(class_labels[i])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"your_project_name\", entity=\"your_entity_name\")\n",
    "\n",
    "# Log images with captions\n",
    "wandb.log({\"sample_images\": [wandb.Image(img, caption=label) for img, label in zip(image_list, image_labels)]})\n",
    "\n",
    "wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrVPfa5wLBTp",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Feed Forword Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68jUzdFWLGFW"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FeedForwardNeuralNetwork:\n",
    "    '''\n",
    "    Neural network model for feedforward architecture.\n",
    "\n",
    "    Attributes:\n",
    "    - hidden_layers (List[int]): List representing the number of neurons in each hidden layer.\n",
    "    - output_layer_neuron (int): Number of neurons in the output layer.\n",
    "    - X_train_raw (numpy.ndarray): Raw training input data.\n",
    "    - Y_train_raw (numpy.ndarray): Raw training output labels.\n",
    "    - N_train (int): Number of training samples.\n",
    "    - X_val_raw (numpy.ndarray): Raw validation input data.\n",
    "    - Y_val_raw (numpy.ndarray): Raw validation output labels.\n",
    "    - N_val (int): Number of validation samples.\n",
    "    - X_test_raw (numpy.ndarray): Raw test input data.\n",
    "    - Y_test_raw (numpy.ndarray): Raw test output labels.\n",
    "    - N_test (int): Number of test samples.\n",
    "    - batch_size (int): Size of the mini-batch used during training.\n",
    "    - weight_decay (float): Weight decay regularization parameter.\n",
    "    - learning_rate (float): Learning rate for optimization.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - activation_fun (str): Activation function used in hidden layers.\n",
    "    - initializer (str): Weight initialization method - \"RANDOM\" (default), \"XAVIER\", or \"HE\".\n",
    "    - optimizer (str): Optimization algorithm - \"SGD\" (default), \"MBGD\", \"NAGD\", \"RMS\", \"ADAM\", or \"NADAM\".\n",
    "    - loss_function (str): Loss function used for training - \"CROSS_ENTROPY\" (default) or MEAN_SQUARE_KEY.\n",
    "\n",
    "    Methods:\n",
    "    - __init__: Initializes the neural network with the provided parameters and initializes weights and biases.\n",
    "    - initializeNeuralNet: Helper function to initialize weights and biases for the neural network layers.\n",
    "\n",
    "    Note:\n",
    "    - The network architecture is defined by the combination of hidden_layers and output_layer_neuron.\n",
    "    - The input data is expected to be flattened, with dimensions (num_features, num_samples).\n",
    "    - Raw input data is normalized to the range [0, 1].\n",
    "    - The activation function and its derivative are specified based on the chosen activation_fun.\n",
    "    - The initializer for weights is selected from \"RANDOM\" (default), \"XAVIER\", or \"HE\".\n",
    "    - The optimization algorithm can be chosen from \"SGD\" (default), \"MBGD\", \"NAGD\", \"RMS\", \"ADAM\", or \"NADAM\".\n",
    "    - The loss function for training is chosen from \"CROSS_ENTROPY\" (default) or MEAN_SQUARE_KEY.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hidden_layers,\n",
    "        num_hidden_neurons,\n",
    "        X_train_raw,\n",
    "        Y_train_raw,\n",
    "        N_train,\n",
    "        X_val_raw,\n",
    "        Y_val_raw,\n",
    "        N_val,\n",
    "        X_test_raw,\n",
    "        Y_test_raw,\n",
    "        N_test,\n",
    "        optimizer,\n",
    "        batch_size,\n",
    "        weight_decay,\n",
    "        learning_rate,\n",
    "        max_epochs,\n",
    "        activation,\n",
    "        initializer,\n",
    "        loss\n",
    "\n",
    "    ):\n",
    "\n",
    "        img_width = 255\n",
    "        self.num_classes = np.max(Y_train_raw) # NUM_CLASSES\n",
    "        self.num_classes += 1\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        print()\n",
    "        self.num_hidden_neurons = num_hidden_neurons\n",
    "        print()\n",
    "        self.output_layer_size = self.num_classes\n",
    "\n",
    "        self.img_wid = X_train_raw.shape[2]\n",
    "\n",
    "        self.img_hei = X_train_raw.shape[1]\n",
    "\n",
    "        self.img_flat_size = self.img_hei * self.img_wid\n",
    "\n",
    "        # self.network = layers\n",
    "        self.network = (\n",
    "            [self.img_flat_size]\n",
    "            + num_hidden_layers * [num_hidden_neurons]\n",
    "            + [self.output_layer_size]\n",
    "        )\n",
    "\n",
    "        self.N_tr = N_train\n",
    "        self.NVal = N_val\n",
    "        self.N_te = N_test\n",
    "\n",
    "        x_raw_param = X_train_raw.reshape(\n",
    "                X_train_raw.shape[0], X_train_raw.shape[1] * X_train_raw.shape[2]\n",
    "            )\n",
    "\n",
    "        self.X_tr = np.transpose(x_raw_param)\n",
    "        x_raw_param = X_test_raw.reshape(\n",
    "                X_test_raw.shape[0], X_test_raw.shape[1] * X_test_raw.shape[2]\n",
    "            )\n",
    "\n",
    "        self.X_te = np.transpose(x_raw_param)\n",
    "        x_raw_param = X_val_raw.reshape(\n",
    "                X_val_raw.shape[0], X_val_raw.shape[1] * X_val_raw.shape[2]\n",
    "            )\n",
    "\n",
    "        self.XVal = np.transpose(x_raw_param)\n",
    "\n",
    "\n",
    "        self.X_te = self.X_te / img_width\n",
    "        self.X_tr = self.X_tr / img_width\n",
    "        self.XVal = self.XVal / img_width\n",
    "\n",
    "        self.Y_train = oneHotEncoding(self.num_classes,Y_train_raw)\n",
    "        self.Y_val = oneHotEncoding(self.num_classes,Y_val_raw)\n",
    "        self.Y_test = oneHotEncoding(self.num_classes,Y_test_raw)\n",
    "\n",
    "\n",
    "        self.Activations_dict = dict([\n",
    "            (SIGMOID_KEY, sigmoid),\n",
    "            (TANH_KEY, tanh),\n",
    "            (RELU_KEY, reLu)\n",
    "            ])\n",
    "        self.DerActivation_dict = dict([\n",
    "            (SIGMOID_KEY, del_sigmoid),\n",
    "            (TANH_KEY, del_tanh),\n",
    "            (RELU_KEY, del_reLu)\n",
    "        ])\n",
    "\n",
    "        self.Optimizer_dict = dict([\n",
    "            (SGD_KEY, self.sgdOptimizer),\n",
    "            (MGD_KEY, self.mgdOptimizer),\n",
    "            (NAG_KEY, self.nagOptimizer),\n",
    "            (RMSPROP_KEY, self.rmsOptimizer),\n",
    "            (ADAM_KEY, self.adamOptimizer),\n",
    "            (NADAM_KEY, self.nadamOptimizer)\n",
    "        ])\n",
    "\n",
    "        self.Initializer_dict = dict([\n",
    "            (XAVIER_KEY, Xavier_initializer),\n",
    "            (RANDOM_KEY, random_initializer),\n",
    "            (HE_KEY, He_initializer)\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.optimizer = self.Optimizer_dict[optimizer]\n",
    "\n",
    "        self.activation = self.Activations_dict[activation]\n",
    "\n",
    "        self.der_activation = self.DerActivation_dict[activation]\n",
    "        print(self.optimizer)\n",
    "        self.loss_function = loss\n",
    "\n",
    "        self.initializer = self.Initializer_dict[initializer]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        print(max_epochs)\n",
    "\n",
    "        self.alpha = learning_rate\n",
    "\n",
    "        init_results = self.initializeNeuralNet(self.network)\n",
    "        self.weights = init_results[0]\n",
    "        self.biases = init_results[1]\n",
    "\n",
    "    def L2RegLoss(self, weight_decay):\n",
    "        '''\n",
    "        Calculates the L2 regularization loss for the neural network weights.\n",
    "\n",
    "        Arguments:\n",
    "        - weight_decay (float): Regularization parameter.\n",
    "\n",
    "        Returns:\n",
    "        - float: L2 regularization loss.\n",
    "        '''\n",
    "        ALPHA = weight_decay\n",
    "        total_norm = 0\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            total_norm += np.linalg.norm(self.weights[str(i + 1)]) ** 2\n",
    "\n",
    "        return ALPHA * total_norm\n",
    "\n",
    "    def predict(self,X,length_dataset):\n",
    "        '''\n",
    "        Generates predictions for a given input dataset.\n",
    "\n",
    "        Arguments:\n",
    "        - X (numpy.ndarray): Input dataset.\n",
    "        - length_dataset (int): Number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Predicted output matrix.\n",
    "        '''\n",
    "        Y_pred = []\n",
    "\n",
    "        for i in range(length_dataset):\n",
    "            Y, H, A = self.forwardPropagate(\n",
    "                X[:, i].reshape(self.img_flat_size, 1),\n",
    "                self.weights,\n",
    "                self.biases,\n",
    "            )\n",
    "            Y_pred.append(Y.reshape(self.num_classes,))\n",
    "\n",
    "        return np.array(Y_pred).T\n",
    "\n",
    "    def initializeNeuralNet(self, layers):\n",
    "        '''\n",
    "        Initializes weights and biases for the neural network layers.\n",
    "\n",
    "        Parameters:\n",
    "        - layers (List[int]): List representing the number of neurons in each layer.\n",
    "\n",
    "        Returns:\n",
    "        - weights (dict): Dictionary containing weight matrices for each layer.\n",
    "        - biases (dict): Dictionary containing bias vectors for each layer.\n",
    "        '''\n",
    "        weights, biases = {}, {}\n",
    "\n",
    "        for l in range(len(layers) - 1):\n",
    "            key = str(l + 1)\n",
    "            weights[key] = self.initializer(dim=[layers[l + 1], layers[l]])\n",
    "            biases[key] = np.zeros((layers[l + 1], 1))\n",
    "\n",
    "        return weights, biases\n",
    "\n",
    "    def initWeight(self):\n",
    "        result = []\n",
    "        for l in range(len(self.network) - 1):\n",
    "            result.append(np.zeros((self.network[l + 1], self.network[l])))\n",
    "        return result\n",
    "\n",
    "    def initBias(self):\n",
    "        result = []\n",
    "        for l in range(len(self.network) - 1):\n",
    "            result.append(np.zeros((self.network[l + 1], 1)))\n",
    "        return result\n",
    "    def forwardPropagate(self, X_train_batch, weights, biases):\n",
    "        \"\"\"\n",
    "        Performs forward propagation to calculate the output of the neural network.\n",
    "\n",
    "        Arguments:\n",
    "        - X_train_batch (numpy.ndarray): Input matrix for a batch of training data.\n",
    "        - weights (dict): Dictionary containing weight matrices for each layer.\n",
    "        - biases (dict): Dictionary containing bias vectors for each layer.\n",
    "\n",
    "        Returns:\n",
    "        - Y_cap (numpy.ndarray): Predicted output matrix for the given input batch.\n",
    "        - H (dict): Dictionary containing activation values for each layer during forward propagation.\n",
    "        - A (dict): Dictionary containing preactivation values for each layer during forward propagation.\n",
    "        \"\"\"\n",
    "        num_layers = len(weights) + 1  # Total number of layers\n",
    "        H, A = {\"0\": X_train_batch}, {\"0\": X_train_batch}  # Initialize activations and preactivations\n",
    "\n",
    "        # Forward propagation for hidden layers\n",
    "        for l in range(1, num_layers - 1):\n",
    "            key = str(l)\n",
    "            W, b = weights[key], biases[key]\n",
    "            A[key] = np.matmul(W, H[str(l - 1)]) + b\n",
    "            H[key] = self.activation(A[key])\n",
    "\n",
    "        # Last layer (output layer, no activation for regression)\n",
    "        last_key = str(num_layers - 1)\n",
    "        W, b = weights[last_key], biases[last_key]\n",
    "        A[last_key] = np.matmul(W, H[str(num_layers - 2)]) + b\n",
    "        Y_cap = softmax(A[last_key])  # Apply softmax to final layer\n",
    "\n",
    "        H[last_key] = Y_cap\n",
    "        return Y_cap, H, A\n",
    "    def backPropagate(self, Y, H, A, Y_train_batch, weight_decay=0):\n",
    "        \"\"\"\n",
    "        Backpropagate through the neural network to compute gradients with respect to weights and biases.\n",
    "\n",
    "        Parameters:\n",
    "        - Y: The predicted output of the neural network.\n",
    "        - H: Dictionary containing hidden layer outputs.\n",
    "        - A: Dictionary containing pre-activation values for each layer.\n",
    "        - Y_train_batch: The true output labels.\n",
    "        - weight_decay: Regularization parameter to control overfitting (default is 0).\n",
    "\n",
    "        Returns:\n",
    "        - gradients_weights: List of weight gradients for each layer.\n",
    "        - gradients_biases: List of bias gradients for each layer.\n",
    "        \"\"\"\n",
    "\n",
    "        num_layers = len(self.network)\n",
    "        gradients_weights, gradients_biases = [], {}\n",
    "\n",
    "        # Compute initial gradient at output layer\n",
    "        grad_A = {}\n",
    "        if self.loss_function == CROSS_ENTROPY_KEY:\n",
    "            grad_A[str(num_layers - 1)] = -(Y_train_batch - Y)\n",
    "        elif self.loss_function == MEAN_SQUARE_KEY:\n",
    "            grad_A[str(num_layers - 1)] = 2 * (Y - Y_train_batch) * Y * (1 - Y)\n",
    "\n",
    "        # Backpropagation loop\n",
    "        for l in range(num_layers - 2, -1, -1):\n",
    "            layer_key = str(l + 1)\n",
    "            prev_layer_key = str(l)\n",
    "\n",
    "            # Compute weight gradients\n",
    "            grad_W = np.outer(grad_A[layer_key], H[prev_layer_key])\n",
    "            if weight_decay != 0:\n",
    "                grad_W += weight_decay * self.weights[layer_key]\n",
    "\n",
    "            # Compute bias gradients\n",
    "            grad_B = grad_A[layer_key]\n",
    "\n",
    "            # Store gradients\n",
    "            gradients_weights.append(grad_W)\n",
    "            gradients_biases[layer_key] = grad_B\n",
    "\n",
    "            # Compute activation gradients for the previous layer\n",
    "            if l > 0:\n",
    "                grad_H = np.matmul(self.weights[layer_key].T, grad_A[layer_key])\n",
    "                grad_A[prev_layer_key] = grad_H * self.der_activation(A[prev_layer_key])\n",
    "            else:\n",
    "                grad_H = np.matmul(self.weights[layer_key].T, grad_A[layer_key])\n",
    "                grad_A[prev_layer_key] = grad_H * A[prev_layer_key]  # No activation function applied\n",
    "\n",
    "        return gradients_weights, list(gradients_biases.values())\n",
    "\n",
    "    def sgd(self, epochs, length_dataset, learning_rate, weight_decay=0):\n",
    "        \"\"\"\n",
    "        Implement Stochastic Gradient Descent (SGD) optimization for training the neural network.\n",
    "\n",
    "        Parameters:\n",
    "        - epochs: Number of training epochs.\n",
    "        - length_dataset: Number of samples in the training dataset.\n",
    "        - learning_rate: Learning rate for the optimization.\n",
    "        - weight_decay: Regularization parameter to control overfitting (default is 0).\n",
    "\n",
    "        Returns:\n",
    "        - train_loss: List of training losses per epoch.\n",
    "        - train_accu: List of training accuracies per epoch.\n",
    "        - val_accu: List of validation accuracies per epoch.\n",
    "        - Y_pred: Predicted outputs after training.\n",
    "        \"\"\"\n",
    "        train_loss, train_accu, val_accu = [], [], []\n",
    "        network_size = len(self.network)\n",
    "\n",
    "        # Extract and reshape training data\n",
    "        X_train, Y_train = self.X_tr[:, :length_dataset], self.Y_train[:, :length_dataset]\n",
    "        X_train = X_train.reshape(self.img_flat_size, length_dataset)\n",
    "        Y_train = Y_train.reshape(self.num_classes, length_dataset)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            LOSS = []\n",
    "\n",
    "            # Initialize weight and bias gradients\n",
    "            del_w, del_b = self.initWeight(), self.initBias()\n",
    "\n",
    "            for i in range(length_dataset):\n",
    "                # Forward pass\n",
    "                Y_cap, H, A = self.forwardPropagate(X_train[:, i].reshape(self.img_flat_size, 1), self.weights, self.biases)\n",
    "\n",
    "                # Backward pass\n",
    "                grad_weights, grad_biases = self.backPropagate(Y_cap, H, A, Y_train[:, i].reshape(self.num_classes, 1))\n",
    "\n",
    "                del_w = grad_weights[::-1]  # Reverse order\n",
    "                del_b = grad_biases[::-1]\n",
    "\n",
    "                # Compute loss with L2 regularization\n",
    "                l2_loss = self.L2RegLoss(weight_decay)\n",
    "                if self.loss_function == MEAN_SQUARE_KEY:\n",
    "                    LOSS.append(meanSquaredErrorLoss(Y_train[:, i].reshape(self.num_classes, 1), Y_cap) + l2_loss)\n",
    "                else:\n",
    "                    LOSS.append(crossEntropyLoss(Y_train[:, i].reshape(self.num_classes, 1), Y_cap) + l2_loss)\n",
    "\n",
    "                # Update weights and biases\n",
    "                for j in range(len(self.weights)):\n",
    "                    self.weights[str(j + 1)] -= learning_rate * del_w[j]\n",
    "\n",
    "                for j in range(len(self.biases)):\n",
    "                    self.biases[str(j + 1)] -= learning_rate * del_b[j]\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            # Compute training accuracy\n",
    "            Y_pred = self.predict(self.X_tr, self.N_tr)\n",
    "            train_loss.append(np.mean(LOSS))\n",
    "            train_accu.append(accuracy(Y_train, Y_pred, length_dataset)[0])\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            Y_val_pred = self.predict(self.XVal, self.NVal)\n",
    "            val_accu.append(accuracy(self.Y_val, Y_val_pred, self.NVal)[0])\n",
    "\n",
    "            # Compute validation loss\n",
    "            l2_reg = self.L2RegLoss(weight_decay)\n",
    "            val_loss = np.mean(\n",
    "                meanSquaredErrorLoss(self.Y_val.T, Y_val_pred.T) + l2_reg\n",
    "                if self.loss_function == MEAN_SQUARE_KEY\n",
    "                else crossEntropyLoss(self.Y_val.T, Y_val_pred.T) + l2_reg\n",
    "            )\n",
    "\n",
    "            printAccuracy(epoch, train_loss[-1], train_accu[-1], val_accu[-1], elapsed_time, self.alpha)\n",
    "\n",
    "        return train_loss, train_accu, val_accu, Y_pred\n",
    "\n",
    "    def sgdOptimizer(self, epochs, length_dataset, batch_size, learning_rate, weight_decay=0):\n",
    "        \"\"\"\n",
    "        Train the neural network using Stochastic Gradient Descent (SGD) with Mini-Batch updates.\n",
    "\n",
    "        Parameters:\n",
    "        - epochs (int): Number of training epochs.\n",
    "        - length_dataset (int): Number of samples in the training dataset.\n",
    "        - batch_size (int): Size of each mini-batch during training.\n",
    "        - learning_rate (float): The learning rate for updating weights and biases.\n",
    "        - weight_decay (float, optional): L2 regularization term to control overfitting (default is 0).\n",
    "\n",
    "        Returns:\n",
    "        - train_loss (list): List of training losses for each epoch.\n",
    "        - train_acc (list): List of training accuracies for each epoch.\n",
    "        - val_accu (list): List of validation accuracies for each epoch.\n",
    "        - Y_pred (numpy array): Predicted labels for the validation set after training.\n",
    "        \"\"\"\n",
    "        X_train, Y_train = self.X_tr[:, :length_dataset], self.Y_train[:, :length_dataset]\n",
    "\n",
    "        train_loss, train_acc, val_accu = [], [], []\n",
    "        num_layers = len(self.network)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Shuffle dataset\n",
    "            indices = np.random.permutation(length_dataset)\n",
    "            X_train, Y_train = X_train[:, indices], Y_train[:, indices]\n",
    "\n",
    "            LOSS = []\n",
    "            deltaw, deltab = self.initWeight(), self.initBias()\n",
    "            num_points_seen = 0\n",
    "\n",
    "            for i in range(length_dataset):\n",
    "                # Forward and backward propagation\n",
    "                Y, H, A = self.forwardPropagate(X_train[:, i].reshape(self.img_flat_size, 1), self.weights, self.biases)\n",
    "                grad_weights, grad_biases = self.backPropagate(Y, H, A, Y_train[:, i].reshape(self.num_classes, 1))\n",
    "\n",
    "                # Accumulate gradients\n",
    "                for j in range(num_layers - 1):\n",
    "                    deltaw[j] += grad_weights[num_layers - 2 - j]\n",
    "                    deltab[j] += grad_biases[num_layers - 2 - j]\n",
    "\n",
    "                # Compute loss with L2 regularization\n",
    "                l2_loss = self.L2RegLoss(weight_decay)\n",
    "                if self.loss_function == MEAN_SQUARE_KEY:\n",
    "                    LOSS.append(meanSquaredErrorLoss(Y_train[:, i].reshape(self.num_classes, 1), Y) + l2_loss)\n",
    "                else:\n",
    "                    LOSS.append(crossEntropyLoss(Y_train[:, i].reshape(self.num_classes, 1), Y) + l2_loss)\n",
    "\n",
    "                num_points_seen += 1\n",
    "\n",
    "                # Update weights and biases at batch interval\n",
    "                if num_points_seen % batch_size == 0:\n",
    "                    self.weights = {str(j + 1): (self.weights[str(j + 1)] - learning_rate * deltaw[j] / batch_size)\n",
    "                                    for j in range(len(self.weights))}\n",
    "                    self.biases = {str(j + 1): (self.biases[str(j + 1)] - learning_rate * deltab[j] / batch_size)\n",
    "                                for j in range(len(self.biases))}\n",
    "\n",
    "                    # Reset gradients\n",
    "                    deltaw, deltab = self.initWeight(), self.initBias()\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            # Compute training and validation metrics\n",
    "            Y_pred = self.predict(self.X_tr, self.N_tr)\n",
    "            train_loss.append(np.mean(LOSS))\n",
    "            train_acc.append(accuracy(Y_train, Y_pred, length_dataset)[0])\n",
    "\n",
    "            Y_val_pred = self.predict(self.XVal, self.NVal)\n",
    "            val_accu.append(accuracy(self.Y_val, Y_val_pred, self.NVal)[0])\n",
    "\n",
    "            # Compute validation loss\n",
    "            l2_reg = self.L2RegLoss(weight_decay)\n",
    "            val_loss = np.mean(meanSquaredErrorLoss(self.Y_val.T, Y_val_pred.T) + l2_reg\n",
    "                            if self.loss_function == MEAN_SQUARE_KEY\n",
    "                            else crossEntropyLoss(self.Y_val.T, Y_val_pred.T) + l2_reg)\n",
    "\n",
    "            printAccuracy(epoch, train_loss[-1], train_acc[-1], val_accu[-1], elapsed_time, self.alpha)\n",
    "\n",
    "        return train_loss, train_acc, val_accu, Y_pred\n",
    "\n",
    "    def mgdOptimizer(self, epochs, length_dataset, batch_size, learning_rate, weight_decay=0):\n",
    "        \"\"\"\n",
    "        Train the neural network using the Mini-Batch Gradient Descent (MGD) optimization algorithm with momentum.\n",
    "\n",
    "        Parameters:\n",
    "        - epochs (int): Number of training epochs.\n",
    "        - length_dataset (int): Number of samples in the training dataset.\n",
    "        - batch_size (int): Size of each mini-batch during training.\n",
    "        - learning_rate (float): The learning rate for updating weights and biases.\n",
    "        - weight_decay (float, optional): L2 regularization term to control overfitting (default is 0).\n",
    "\n",
    "        Returns:\n",
    "        - train_loss (list): List of training losses for each epoch.\n",
    "        - train_acc (list): List of training accuracies for each epoch.\n",
    "        - val_accu (list): List of validation accuracies for each epoch.\n",
    "        - Y_pred (numpy array): Predicted labels for the validation set after training.\n",
    "        \"\"\"\n",
    "\n",
    "        GAMMA = 0.9  # Momentum factor\n",
    "\n",
    "        X_train, Y_train = self.X_tr[:, :length_dataset], self.Y_train[:, :length_dataset]\n",
    "\n",
    "        train_loss, train_acc, val_accu = [], [], []\n",
    "        num_layers = len(self.network)\n",
    "\n",
    "        # Initialize previous velocity values for momentum\n",
    "        prev_v_w, prev_v_b = self.initWeight(), self.initBias()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Shuffle dataset\n",
    "            indices = np.random.permutation(length_dataset)\n",
    "            X_train, Y_train = X_train[:, indices], Y_train[:, indices]\n",
    "\n",
    "            LOSS = []\n",
    "            deltaw, deltab = self.initWeight(), self.initBias()\n",
    "            num_points_seen = 0\n",
    "\n",
    "            for i in range(length_dataset):\n",
    "                # Forward and backward propagation\n",
    "                Y, H, A = self.forwardPropagate(X_train[:, i].reshape(self.img_flat_size, 1), self.weights, self.biases)\n",
    "                grad_weights, grad_biases = self.backPropagate(Y, H, A, Y_train[:, i].reshape(self.num_classes, 1))\n",
    "\n",
    "                # Accumulate gradients\n",
    "                for j in range(num_layers - 1):\n",
    "                    deltaw[j] += grad_weights[num_layers - 2 - j]\n",
    "                    deltab[j] += grad_biases[num_layers - 2 - j]\n",
    "\n",
    "                # Compute loss with L2 regularization\n",
    "                l2_loss = self.L2RegLoss(weight_decay)\n",
    "                if self.loss_function == MEAN_SQUARE_KEY:\n",
    "                    LOSS.append(meanSquaredErrorLoss(Y_train[:, i].reshape(self.num_classes, 1), Y) + l2_loss)\n",
    "                else:\n",
    "                    LOSS.append(crossEntropyLoss(Y_train[:, i].reshape(self.num_classes, 1), Y) + l2_loss)\n",
    "\n",
    "                num_points_seen += 1\n",
    "\n",
    "                # Update weights and biases at batch interval with momentum\n",
    "                if num_points_seen % batch_size == 0:\n",
    "                    v_w = [GAMMA * prev_v_w[j] + learning_rate * deltaw[j] / batch_size for j in range(num_layers - 1)]\n",
    "                    v_b = [GAMMA * prev_v_b[j] + learning_rate * deltab[j] / batch_size for j in range(num_layers - 1)]\n",
    "\n",
    "                    # Update weights and biases\n",
    "                    self.weights = {str(j + 1): self.weights[str(j + 1)] - v_w[j] for j in range(len(self.weights))}\n",
    "                    self.biases = {str(j + 1): self.biases[str(j + 1)] - v_b[j] for j in range(len(self.biases))}\n",
    "\n",
    "                    # Store previous velocity\n",
    "                    prev_v_w, prev_v_b = v_w, v_b\n",
    "\n",
    "                    # Reset accumulated gradients\n",
    "                    deltaw, deltab = self.initWeight(), self.initBias()\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            # Compute training and validation metrics\n",
    "            Y_pred = self.predict(self.X_tr, self.N_tr)\n",
    "            train_loss.append(np.mean(LOSS))\n",
    "            train_acc.append(accuracy(Y_train, Y_pred, length_dataset)[0])\n",
    "\n",
    "            Y_val_pred = self.predict(self.XVal, self.NVal)\n",
    "            val_accu.append(accuracy(self.Y_val, Y_val_pred, self.NVal)[0])\n",
    "\n",
    "            # Compute validation loss\n",
    "            l2_reg = self.L2RegLoss(weight_decay)\n",
    "            val_loss = np.mean(meanSquaredErrorLoss(self.Y_val.T, Y_val_pred.T) + l2_reg\n",
    "                            if self.loss_function == MEAN_SQUARE_KEY\n",
    "                            else crossEntropyLoss(self.Y_val.T, Y_val_pred.T) + l2_reg)\n",
    "\n",
    "            printAccuracy(epoch, train_loss[-1], train_acc[-1], val_accu[-1], elapsed_time, self.alpha)\n",
    "\n",
    "        return train_loss, train_acc, val_accu, Y_pred\n",
    "\n",
    "    def nagOptimizer(self, epochs, length_dataset, batch_size, learning_rate, weight_decay=0):\n",
    "        \"\"\"\n",
    "        Train the neural network using the Nesterov Accelerated Gradient (NAG) optimization algorithm.\n",
    "        \"\"\"\n",
    "        GAMMA = 0.9\n",
    "        X_train, Y_train = self.X_tr[:, :length_dataset], self.Y_train[:, :length_dataset]\n",
    "\n",
    "        train_loss, train_acc, val_accu = [], [], []\n",
    "        num_layers = len(self.network)\n",
    "        prev_v_w, prev_v_b = self.initWeight(), self.initBias()\n",
    "        num_points_seen, epoch = 0, 0\n",
    "\n",
    "        while epoch < epochs:\n",
    "            start_time = time.time()\n",
    "            offset = np.random.permutation(length_dataset)\n",
    "            X_train, Y_train = X_train[:, offset], Y_train[:, offset]\n",
    "\n",
    "            LOSS, deltaw, deltab = [], self.initWeight(), self.initBias()\n",
    "            v_w = [GAMMA * prev_v_w[l] for l in range(num_layers - 1)]\n",
    "            v_b = [GAMMA * prev_v_b[l] for l in range(num_layers - 1)]\n",
    "            \n",
    "            for i in range(length_dataset):\n",
    "                winter = {str(l + 1): self.weights[str(l + 1)] - v_w[l] for l in range(num_layers - 1)}\n",
    "                binter = {str(l + 1): self.biases[str(l + 1)] - v_b[l] for l in range(num_layers - 1)}\n",
    "                \n",
    "                Y, H, A = self.forwardPropagate(X_train[:, i].reshape(self.img_flat_size, 1), winter, binter)\n",
    "                grad_weights, grad_biases = self.backPropagate(Y, H, A, Y_train[:, i].reshape(self.num_classes, 1))\n",
    "                \n",
    "                for l in range(num_layers - 1):\n",
    "                    deltaw[l] += grad_weights[num_layers - 2 - l]\n",
    "                    deltab[l] += grad_biases[num_layers - 2 - l]\n",
    "                \n",
    "                loss_fn = meanSquaredErrorLoss if self.loss_function == MEAN_SQUARE_KEY else crossEntropyLoss\n",
    "                LOSS.append(loss_fn(Y_train[:, i].reshape(self.num_classes, 1), Y) + self.L2RegLoss(weight_decay))\n",
    "                \n",
    "                num_points_seen += 1\n",
    "                if num_points_seen % batch_size == 0:\n",
    "                    v_w = [GAMMA * prev_v_w[l] + learning_rate * deltaw[l] / batch_size for l in range(num_layers - 1)]\n",
    "                    v_b = [GAMMA * prev_v_b[l] + learning_rate * deltab[l] / batch_size for l in range(num_layers - 1)]\n",
    "                    \n",
    "                    for l in range(num_layers - 1):\n",
    "                        self.weights[str(l + 1)] -= v_w[l]\n",
    "                        self.biases[str(l + 1)] -= v_b[l]\n",
    "                    \n",
    "                    prev_v_w, prev_v_b = v_w, v_b\n",
    "                    deltaw, deltab = self.initWeight(), self.initBias()\n",
    "            \n",
    "            Y_pred = self.predict(self.X_tr, self.N_tr)\n",
    "            train_loss.append(np.mean(LOSS))\n",
    "            train_acc.append(accuracy(Y_train, Y_pred, length_dataset)[0])\n",
    "            \n",
    "            Y_val_pred = self.predict(self.XVal, self.NVal)\n",
    "            val_accu.append(accuracy(self.Y_val, Y_val_pred, self.NVal)[0])\n",
    "            \n",
    "            loss_fn = meanSquaredErrorLoss if self.loss_function == MEAN_SQUARE_KEY else crossEntropyLoss\n",
    "            val_loss = np.mean(loss_fn(self.Y_val.T, Y_val_pred.T) + self.L2RegLoss(weight_decay))\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            printAccuracy(epoch, train_loss[epoch], train_acc[epoch], val_accu[epoch], elapsed, self.alpha)\n",
    "            epoch += 1\n",
    "\n",
    "        return train_loss, train_acc, val_accu, Y_pred\n",
    "\n",
    "    def rmsOptimizer(self, epochs, length_dataset, batch_size, learning_rate, weight_decay=0):\n",
    "        \"\"\"\n",
    "        Train the neural network using the RMSProp optimization algorithm.\n",
    "        \"\"\"\n",
    "        X_train, Y_train = self.X_tr[:, :length_dataset], self.Y_train[:, :length_dataset]\n",
    "        train_loss, train_acc, val_accu = [], [], []\n",
    "        num_layers, EPS, BETA = len(self.network), 1e-8, 0.9\n",
    "        v_w, v_b = self.initWeight(), self.initBias()\n",
    "        num_points_seen, epoch = 0, 0\n",
    "\n",
    "        while epoch < epochs:\n",
    "            start_time = time.time()\n",
    "            offset = np.random.permutation(length_dataset)\n",
    "            X_train, Y_train = X_train[:, offset], Y_train[:, offset]\n",
    "            LOSS, deltaw, deltab = [], self.initWeight(), self.initBias()\n",
    "\n",
    "            for i in range(length_dataset):\n",
    "                Y, H, A = self.forwardPropagate(X_train[:, i].reshape(self.img_flat_size, 1), self.weights, self.biases)\n",
    "                grad_weights, grad_biases = self.backPropagate(Y, H, A, Y_train[:, i].reshape(self.num_classes, 1))\n",
    "                deltaw = [grad_weights[num_layers - 2 - j] + deltaw[j] for j in range(num_layers - 1)]\n",
    "                deltab = [grad_biases[num_layers - 2 - j] + deltab[j] for j in range(num_layers - 1)]\n",
    "                \n",
    "                loss_value = (meanSquaredErrorLoss(Y_train[:, i].reshape(self.num_classes, 1), Y) if self.loss_function == MEAN_SQUARE_KEY\n",
    "                              else crossEntropyLoss(Y_train[:, i].reshape(self.num_classes, 1), Y))\n",
    "                LOSS.append(loss_value + self.L2RegLoss(weight_decay))\n",
    "                num_points_seen += 1\n",
    "                \n",
    "                if num_points_seen % batch_size == 0:\n",
    "                    v_w = [BETA * v_w[j] + (1 - BETA) * deltaw[j] ** 2 for j in range(num_layers - 1)]\n",
    "                    v_b = [BETA * v_b[j] + (1 - BETA) * deltab[j] ** 2 for j in range(num_layers - 1)]\n",
    "                    \n",
    "                    self.weights = {str(j + 1): self.weights[str(j + 1)] - (learning_rate / np.sqrt(v_w[j] + EPS)) * deltaw[j]\n",
    "                                    for j in range(len(self.weights))}\n",
    "                    self.biases = {str(j + 1): self.biases[str(j + 1)] - (learning_rate / np.sqrt(v_b[j] + EPS)) * deltab[j]\n",
    "                                   for j in range(len(self.biases))}\n",
    "                    deltaw, deltab = self.initWeight(), self.initBias()\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            Y_pred, Y_val_pred = self.predict(self.X_tr, self.N_tr), self.predict(self.XVal, self.NVal)\n",
    "            train_loss.append(np.mean(LOSS))\n",
    "            train_acc.append(accuracy(Y_train, Y_pred, length_dataset)[0])\n",
    "            val_accu.append(accuracy(self.Y_val, Y_val_pred, self.NVal)[0])\n",
    "            val_loss = np.mean((meanSquaredErrorLoss(self.Y_val.T, Y_val_pred.T) if self.loss_function == MEAN_SQUARE_KEY\n",
    "                                else crossEntropyLoss(self.Y_val.T, Y_val_pred.T)) + self.L2RegLoss(weight_decay))\n",
    "            printAccuracy(epoch, train_loss[epoch], train_acc[epoch], val_accu[epoch], elapsed, self.alpha)\n",
    "            epoch += 1\n",
    "        \n",
    "        return train_loss, train_acc, val_accu, Y_pred\n",
    "\n",
    "    def adamOptimizer(self, epochs, length_dataset, batch_size, learning_rate, weight_decay=0):\n",
    "        \"\"\"\n",
    "        Train the neural network using the Adam optimization algorithm.\n",
    "        \"\"\"\n",
    "        X_train, Y_train = self.X_tr[:, :length_dataset], self.Y_train[:, :length_dataset]\n",
    "        train_loss, train_acc, val_accu = [], [], []\n",
    "        num_layers = len(self.network)\n",
    "        EPS, BETA1, BETA2 = 1e-8, 0.9, 0.99\n",
    "        \n",
    "        # Initialize moment estimates\n",
    "        m_w, m_b = self.initWeight(), self.initBias()\n",
    "        v_w, v_b = self.initWeight(), self.initBias()\n",
    "        \n",
    "        num_points_seen, epoch = 0, 0\n",
    "        while epoch < epochs:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Shuffle dataset\n",
    "            indices = np.random.permutation(length_dataset)\n",
    "            X_train, Y_train = X_train[:, indices], Y_train[:, indices]\n",
    "            \n",
    "            loss_per_epoch = []\n",
    "            deltaw, deltab = self.initWeight(), self.initBias()\n",
    "            \n",
    "            for i in range(length_dataset):\n",
    "                Y, H, A = self.forwardPropagate(X_train[:, i].reshape(self.img_flat_size, 1), self.weights, self.biases)\n",
    "                grad_weights, grad_biases = self.backPropagate(Y, H, A, Y_train[:, i].reshape(self.num_classes, 1))\n",
    "                \n",
    "                for j in range(num_layers - 1):\n",
    "                    deltaw[j] += grad_weights[num_layers - 2 - j]\n",
    "                    deltab[j] += grad_biases[num_layers - 2 - j]\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.L2RegLoss(weight_decay) + (\n",
    "                    meanSquaredErrorLoss(Y_train[:, i].reshape(self.num_classes, 1), Y)\n",
    "                    if self.loss_function == MEAN_SQUARE_KEY else\n",
    "                    crossEntropyLoss(Y_train[:, i].reshape(self.num_classes, 1), Y)\n",
    "                )\n",
    "                loss_per_epoch.append(loss)\n",
    "                \n",
    "                num_points_seen += 1\n",
    "                \n",
    "                if num_points_seen % batch_size == 0:\n",
    "                    # Update moment estimates\n",
    "                    for l in range(num_layers - 1):\n",
    "                        m_w[l] = BETA1 * m_w[l] + (1 - BETA1) * deltaw[l]\n",
    "                        m_b[l] = BETA1 * m_b[l] + (1 - BETA1) * deltab[l]\n",
    "                        v_w[l] = BETA2 * v_w[l] + (1 - BETA2) * (deltaw[l] ** 2)\n",
    "                        v_b[l] = BETA2 * v_b[l] + (1 - BETA2) * (deltab[l] ** 2)\n",
    "                        \n",
    "                    # Bias correction\n",
    "                    m_w_hat = [mw / (1 - BETA1 ** (epoch + 1)) for mw in m_w]\n",
    "                    m_b_hat = [mb / (1 - BETA1 ** (epoch + 1)) for mb in m_b]\n",
    "                    v_w_hat = [vw / (1 - BETA2 ** (epoch + 1)) for vw in v_w]\n",
    "                    v_b_hat = [vb / (1 - BETA2 ** (epoch + 1)) for vb in v_b]\n",
    "                    \n",
    "                    # Parameter update\n",
    "                    for l in range(len(self.weights)):\n",
    "                        self.weights[str(l + 1)] -= (learning_rate / np.sqrt(v_w_hat[l] + EPS)) * m_w_hat[l]\n",
    "                        self.biases[str(l + 1)] -= (learning_rate / np.sqrt(v_b_hat[l] + EPS)) * m_b_hat[l]\n",
    "                    \n",
    "                    deltaw, deltab = self.initWeight(), self.initBias()\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            # Evaluate accuracy\n",
    "            Y_pred = self.predict(self.X_tr, self.N_tr)\n",
    "            train_loss.append(np.mean(loss_per_epoch))\n",
    "            train_acc.append(accuracy(Y_train, Y_pred, length_dataset)[0])\n",
    "            \n",
    "            Y_val_pred = self.predict(self.XVal, self.NVal)\n",
    "            val_accu.append(accuracy(self.Y_val, Y_val_pred, self.NVal)[0])\n",
    "            \n",
    "            val_loss = np.mean(\n",
    "                meanSquaredErrorLoss(self.Y_val.T, Y_val_pred.T) if self.loss_function == MEAN_SQUARE_KEY else\n",
    "                crossEntropyLoss(self.Y_val.T, Y_val_pred.T)\n",
    "            ) + self.L2RegLoss(weight_decay)\n",
    "            \n",
    "            printAccuracy(epoch, train_loss[epoch], train_acc[epoch], val_accu[epoch], elapsed, self.alpha)\n",
    "            epoch += 1\n",
    "        \n",
    "        return train_loss, train_acc, val_accu, Y_pred\n",
    "\n",
    "    def nadamOptimizer(self, epochs, length_dataset, batch_size, learning_rate, weight_decay=0):\n",
    "        \"\"\"\n",
    "        Train the neural network using the Nadam optimization algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - epochs (int): Number of training epochs.\n",
    "        - length_dataset (int): Number of samples in the training dataset.\n",
    "        - batch_size (int): Size of each mini-batch during training.\n",
    "        - learning_rate (float): Learning rate for updating weights and biases.\n",
    "        - weight_decay (float, optional): L2 regularization term to control overfitting (default is 0).\n",
    "\n",
    "        Returns:\n",
    "        - train_loss (list): List of training losses for each epoch.\n",
    "        - train_acc (list): List of training accuracies for each epoch.\n",
    "        - val_accu (list): List of validation accuracies for each epoch.\n",
    "        - Y_pred (numpy array): Predicted labels for the validation set after training.\n",
    "        \"\"\"\n",
    "        X_train, Y_train = self.X_tr[:, :length_dataset], self.Y_train[:, :length_dataset]\n",
    "\n",
    "        train_loss, train_acc, val_accu = [], [], []\n",
    "        num_layers = len(self.network)\n",
    "\n",
    "        # Nadam hyperparameters\n",
    "        EPS, BETA1, BETA2 = 1e-8, 0.9, 0.99\n",
    "\n",
    "        # Initialize momentum and velocity terms\n",
    "        m_w, m_b = self.initWeight(), self.initBias()\n",
    "        v_w, v_b = self.initWeight(), self.initBias()\n",
    "\n",
    "        num_points_seen, epoch = 0, 0\n",
    "\n",
    "        while epoch < epochs:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Shuffle the dataset\n",
    "            indices = np.random.permutation(length_dataset)\n",
    "            X_train, Y_train = X_train[:, indices], Y_train[:, indices]\n",
    "\n",
    "            LOSS, deltaw, deltab = [], self.initWeight(), self.initBias()\n",
    "\n",
    "            for i in range(length_dataset):\n",
    "                Y, H, A = self.forwardPropagate(X_train[:, i].reshape(self.img_flat_size, 1), self.weights, self.biases)\n",
    "                grad_weights, grad_biases = self.backPropagate(Y, H, A, Y_train[:, i].reshape(self.num_classes, 1))\n",
    "\n",
    "                # Accumulate gradients\n",
    "                deltaw = [grad_weights[j] + deltaw[j] for j in range(num_layers - 1)]\n",
    "                deltab = [grad_biases[j] + deltab[j] for j in range(num_layers - 1)]\n",
    "\n",
    "                # Compute loss with L2 regularization\n",
    "                reg_term = self.L2RegLoss(weight_decay)\n",
    "                loss_fn = meanSquaredErrorLoss if self.loss_function == MEAN_SQUARE_KEY else crossEntropyLoss\n",
    "                LOSS.append(loss_fn(Y_train[:, i].reshape(self.num_classes, 1), Y) + reg_term)\n",
    "\n",
    "                num_points_seen += 1\n",
    "\n",
    "                # Apply updates at the end of a mini-batch\n",
    "                if num_points_seen % batch_size == 0:\n",
    "                    # Update biased first moments (momentum)\n",
    "                    m_w = [BETA1 * m_w[l] + (1 - BETA1) * deltaw[l] for l in range(num_layers - 1)]\n",
    "                    m_b = [BETA1 * m_b[l] + (1 - BETA1) * deltab[l] for l in range(num_layers - 1)]\n",
    "\n",
    "                    # Update biased second moments (velocity)\n",
    "                    v_w = [BETA2 * v_w[l] + (1 - BETA2) * (deltaw[l] ** 2) for l in range(num_layers - 1)]\n",
    "                    v_b = [BETA2 * v_b[l] + (1 - BETA2) * (deltab[l] ** 2) for l in range(num_layers - 1)]\n",
    "\n",
    "                    # Compute bias-corrected estimates\n",
    "                    m_w_hat = [m_w[l] / (1 - BETA1 ** (epoch + 1)) for l in range(num_layers - 1)]\n",
    "                    m_b_hat = [m_b[l] / (1 - BETA1 ** (epoch + 1)) for l in range(num_layers - 1)]\n",
    "                    v_w_hat = [v_w[l] / (1 - BETA2 ** (epoch + 1)) for l in range(num_layers - 1)]\n",
    "                    v_b_hat = [v_b[l] / (1 - BETA2 ** (epoch + 1)) for l in range(num_layers - 1)]\n",
    "\n",
    "                    # Compute final parameter updates\n",
    "                    self.weights = {\n",
    "                        str(l + 1): self.weights[str(l + 1)] - (learning_rate / (np.sqrt(v_w_hat[l]) + EPS)) *\n",
    "                        (BETA1 * m_w_hat[l] + (1 - BETA1) * deltaw[l])\n",
    "                        for l in range(len(self.weights))\n",
    "                    }\n",
    "                    self.biases = {\n",
    "                        str(l + 1): self.biases[str(l + 1)] - (learning_rate / (np.sqrt(v_b_hat[l]) + EPS)) *\n",
    "                        (BETA1 * m_b_hat[l] + (1 - BETA1) * deltab[l])\n",
    "                        for l in range(len(self.biases))\n",
    "                    }\n",
    "\n",
    "                    # Reset gradients\n",
    "                    deltaw, deltab = self.initWeight(), self.initBias()\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            # Evaluate model performance\n",
    "            Y_pred = self.predict(self.X_tr, self.N_tr)\n",
    "            train_loss.append(np.mean(LOSS))\n",
    "            train_acc.append(accuracy(Y_train, Y_pred, length_dataset)[0])\n",
    "            Y_val_pred = self.predict(self.XVal, self.NVal)\n",
    "            val_accu.append(accuracy(self.Y_val, Y_val_pred, self.NVal)[0])\n",
    "\n",
    "            # Compute validation loss\n",
    "            reg_term = self.L2RegLoss(weight_decay)\n",
    "            loss_fn = meanSquaredErrorLoss if self.loss_function == MEAN_SQUARE_KEY else crossEntropyLoss\n",
    "            val_loss = np.mean(loss_fn(self.Y_val.T, Y_val_pred.T) + reg_term)\n",
    "\n",
    "            printAccuracy(epoch, train_loss[epoch], train_acc[epoch], val_accu[epoch], elapsed, self.alpha)\n",
    "            epoch += 1\n",
    "\n",
    "        return train_loss, train_acc, val_accu, Y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0jrDTM5cOjg",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Loading and transforming data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqLb8E_5LsPT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "\n",
    "(trainIn, trainOut), (testIn, testOut) = fashion_mnist.load_data()\n",
    "\n",
    "N_train_full = trainOut.shape[0]\n",
    "N_train = int(0.9*N_train_full)\n",
    "N_validation = int(0.1 * trainOut.shape[0])\n",
    "N_test = testOut.shape[0]\n",
    "\n",
    "\n",
    "idx  = np.random.choice(trainOut.shape[0], N_train_full, replace=False)\n",
    "idx2 = np.random.choice(testOut.shape[0], N_test, replace=False)\n",
    "\n",
    "trainInFull = trainIn[idx, :]\n",
    "trainOutFull = trainOut[idx]\n",
    "\n",
    "trainIn = trainInFull[:N_train,:]\n",
    "trainOut = trainOutFull[:N_train]\n",
    "\n",
    "validIn = trainInFull[N_train:, :]\n",
    "validOut = trainOutFull[N_train:]\n",
    "\n",
    "testIn = testIn[idx2, :]\n",
    "testOut = testOut[idx2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84tT6I2ILyve",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCMRE1_zLoEy"
   },
   "outputs": [],
   "source": [
    "FFNN = FeedForwardNeuralNetwork(\n",
    "    num_hidden_layers=3,\n",
    "    num_hidden_neurons=128,\n",
    "    X_train_raw=trainInFull,\n",
    "    Y_train_raw=trainOutFull,\n",
    "    N_train = N_train_full,\n",
    "    X_val_raw = validIn,\n",
    "    Y_val_raw = validOut,\n",
    "    N_val = N_validation,\n",
    "    X_test_raw = testIn,\n",
    "    Y_test_raw = testOut,\n",
    "    N_test = N_test,\n",
    "    optimizer = NADAM_KEY,\n",
    "    batch_size = 32,\n",
    "    weight_decay = 0,\n",
    "    learning_rate = 1e-3,\n",
    "    max_epochs = 10,\n",
    "    activation = RELU_KEY,\n",
    "    initializer = XAVIER_KEY,\n",
    "    loss = CROSS_ENTROPY_KEY\n",
    "    )\n",
    "\n",
    "X_train = trainInFull.reshape(\n",
    "    trainInFull.shape[1]*trainInFull.shape[2], trainInFull.shape[0]\n",
    ")\n",
    "\n",
    "Y_cap,H,A = FFNN.forwardPropagate(X_train,FFNN.weights,FFNN.biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27P-DgKlN03t",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTX2BS55N_ch"
   },
   "outputs": [],
   "source": [
    "FFNN = FeedForwardNeuralNetwork(\n",
    "    num_hidden_layers=3,\n",
    "    num_hidden_neurons=128,\n",
    "    X_train_raw=trainInFull,\n",
    "    Y_train_raw=trainOutFull,\n",
    "    N_train = N_train_full,\n",
    "    X_val_raw = validIn,\n",
    "    Y_val_raw = validOut,\n",
    "    N_val = N_validation,\n",
    "    X_test_raw = testIn,\n",
    "    Y_test_raw = testOut,\n",
    "    N_test = N_test,\n",
    "    optimizer = NADAM_KEY,\n",
    "    batch_size = 32,\n",
    "    weight_decay = 0,\n",
    "    learning_rate = 1e-3,\n",
    "    max_epochs = 10,\n",
    "    activation = RELU_KEY,\n",
    "    initializer = XAVIER_KEY,\n",
    "    loss = CROSS_ENTROPY_KEY\n",
    "    )\n",
    "\n",
    "training_loss, trainingaccuracy, validationaccuracy, Y_pred_train = FFNN.optimizer(FFNN.max_epochs, FFNN.N_train, FFNN.batch_size, FFNN.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7isu6GCOXzb"
   },
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yv1qncnWLsPY",
    "outputId": "6578b74e-9cd8-4a2b-8b57-56efa76511c0"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Define sweep configuration\n",
    "sweep_config = {\n",
    "    \"name\": \"Bayesian Sweep\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"max_epochs\": {\"values\": [5, 10]},\n",
    "        \"initializer\": {\"values\": [RANDOM_KEY, XAVIER_KEY, HE_KEY]},\n",
    "        \"num_layers\": {\"values\": [3, 4, 5]},\n",
    "        \"num_hidden_neurons\": {\"values\": [32, 64, 128]},\n",
    "        \"activation\": {\"values\": [TANH_KEY, SIGMOID_KEY, RELU_KEY]},\n",
    "        \"learning_rate\": {\"values\": [0.001, 0.0001]},\n",
    "        \"weight_decay\": {\"values\": [0, 0.0005, 0.5]},\n",
    "        \"optimizer\": {\"values\": [SGD_KEY, MGD_KEY, NAG_KEY, RMSPROP_KEY, ADAM_KEY, NADAM_KEY]},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"dl-assignment-1\", entity=PROJECT_ENTITY)\n",
    "\n",
    "def train():\n",
    "    \"\"\"Train function for WandB sweep.\"\"\"\n",
    "    config_defaults = {\n",
    "        \"max_epochs\": 10,\n",
    "        \"num_hidden_layers\": 2,\n",
    "        \"num_hidden_neurons\": 128,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"optimizer\": NADAM_KEY,\n",
    "        \"batch_size\": 32,\n",
    "        \"activation\": SIGMOID_KEY,\n",
    "        \"initializer\": HE_KEY,\n",
    "        \"loss\": CROSS_ENTROPY_KEY,\n",
    "    }\n",
    "\n",
    "    # Initialize WandB run\n",
    "    wandb.init(project=\"dl-assignment-1\", entity=PROJECT_ENTITY, config=config_defaults)\n",
    "    config = wandb.config\n",
    "\n",
    "    # Set a descriptive run name\n",
    "    wandb.run.name = (\n",
    "        f\"CROSS_hl_{config.num_hidden_layers}_hn_{config.num_hidden_neurons}\"\n",
    "        f\"_opt_{config.optimizer}_act_{config.activation}_lr_{config.learning_rate}\"\n",
    "        f\"_bs_{config.batch_size}_init_{config.initializer}_ep_{config.max_epochs}\"\n",
    "        f\"_l2_{config.weight_decay}\"\n",
    "    )\n",
    "\n",
    "    # Initialize neural network model\n",
    "    FFNN = FeedForwardNeuralNetwork(\n",
    "        num_hidden_layers=config.num_hidden_layers,\n",
    "        num_hidden_neurons=config.num_hidden_neurons,\n",
    "        X_train_raw=trainIn,\n",
    "        Y_train_raw=trainOut,\n",
    "        N_train=N_train,\n",
    "        X_val_raw=validIn,\n",
    "        Y_val_raw=validOut,\n",
    "        N_val=N_validation,\n",
    "        X_test_raw=testIn,\n",
    "        Y_test_raw=testOut,\n",
    "        N_test=N_test,\n",
    "        optimizer=config.optimizer,\n",
    "        batch_size=config.batch_size,\n",
    "        weight_decay=config.weight_decay,\n",
    "        learning_rate=config.learning_rate,\n",
    "        max_epochs=config.max_epochs,\n",
    "        activation=config.activation,\n",
    "        initializer=config.initializer,\n",
    "        loss=config.loss,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    training_loss, training_accuracy, validation_accuracy, Y_pred_train = FFNN.optimizer(\n",
    "        FFNN.max_epochs, FFNN.N_train, FFNN.batch_size, FFNN.alpha\n",
    "    )\n",
    "\n",
    "# Run WandB agent for sweep optimization\n",
    "wandb.agent(sweep_id, train, count=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sI1UwISILwJO",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Question - 7\n",
    "code for confussion matrix with best params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 875,
     "referenced_widgets": [
      "ff0e6f7f547e457992c173536cd1a7eb",
      "a1e8719afeec43cda8540476a5cbdffe",
      "63caec8292094fe19acf9b6dcde0a5ed",
      "07e4163d41bd4aaebae2a76ccbb767a7",
      "e0a71a3f9fc848319aae3cbf73de0d22",
      "9dc3b7c5a2e64d399e582f51d1c565dd",
      "ee57ab04d4d34ed987c2577756ec8bbe",
      "9a9d8af854d34e3486727e81a701da30",
      "4fddc31e9c784ae9a6cf76277f004f27",
      "7bc9827bf21c4cebb76408db886c397c",
      "cdd58212b57d4a789b5cf6ff3d4edbf7",
      "74aa8445b46e4002821ce45d3a31956b",
      "d70e938fc75149de8717ad126b261fd4",
      "9d5a1f51bd1b4c8a88be3f606510490e",
      "1a093c18d82f43af94229922b645532f",
      "f0f35a8e969643b18eb62680fd9ba6c7",
      "1eec5dbf0fd04358b9073dce5268ca27",
      "b0d2e5b583014d94895475b9e64a54fe",
      "58894764e2354d73a77542b08f384940",
      "1c8e87e890224abca869161f20ff75e3",
      "9d2705f0916c491d8bfefbb2c925681d",
      "c0a7112a248041a4b2fba524ad2aa38f",
      "258c07dded5b481587d4de2fa71b6064",
      "b16efa46a2c64d4cb164458b05ef7df4",
      "6187f791e4b84b8396c130b752ab73ab",
      "2d0df6063f8f4838bea906dc8ef9bfd6",
      "26ebecd763da4b8baddb829a24af7fe9",
      "1aa7b4c7f2044519b8f7e3d4bfbcb2ad",
      "b59ac281ebf8412da8608ad546cd2371",
      "1a2ef16481e94ca3bf877b27988b13c2",
      "ac505941d2844e47adcbee09a8f44f81",
      "716e275fa6a94454b70212d91e93ddde"
     ]
    },
    "id": "VVgruYt6CJl6",
    "outputId": "8c03687d-315d-4545-974d-a5d0faa3fad6"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def train_and_test():\n",
    "    \"\"\"Train and evaluate the best model configuration.\"\"\"\n",
    "    best_config = {\n",
    "        \"max_epochs\": 10,\n",
    "        \"num_hidden_layers\": 3,\n",
    "        \"num_hidden_neurons\": 128,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"optimizer\": NADAM_KEY,\n",
    "        \"batch_size\": 32,\n",
    "        \"activation\": RELU_KEY,\n",
    "        \"initializer\": XAVIER_KEY,\n",
    "        \"loss\": CROSS_ENTROPY_KEY,\n",
    "    }\n",
    "\n",
    "    # Initialize WandB\n",
    "    wandb.init(project=PROJECT_NAME_KEY, entity=PROJECT_ENTITY, config=best_config)\n",
    "    \n",
    "    # Set a descriptive run name\n",
    "    wandb.run.name = (\n",
    "        f\"best_{wandb.config.num_hidden_layers}_hn_{wandb.config.num_hidden_neurons}\"\n",
    "        f\"_opt_{wandb.config.optimizer}_act_{wandb.config.activation}\"\n",
    "        f\"_lr_{wandb.config.learning_rate}_bs_{wandb.config.batch_size}\"\n",
    "        f\"_init_{wandb.config.initializer}_ep_{wandb.config.max_epochs}\"\n",
    "        f\"_l2_{wandb.config.weight_decay}\"\n",
    "    )\n",
    "\n",
    "    config = wandb.config\n",
    "\n",
    "    # Initialize neural network model\n",
    "    FFNN = FeedForwardNeuralNetwork(\n",
    "        num_hidden_layers=config.num_hidden_layers,\n",
    "        num_hidden_neurons=config.num_hidden_neurons,\n",
    "        X_train_raw=trainInFull,\n",
    "        Y_train_raw=trainOutFull,\n",
    "        N_train=N_train_full,\n",
    "        X_val_raw=validIn,\n",
    "        Y_val_raw=validOut,\n",
    "        N_val=N_validation,\n",
    "        X_test_raw=testIn,\n",
    "        Y_test_raw=testOut,\n",
    "        N_test=N_test,\n",
    "        optimizer=config.optimizer,\n",
    "        batch_size=config.batch_size,\n",
    "        weight_decay=config.weight_decay,\n",
    "        learning_rate=config.learning_rate,\n",
    "        max_epochs=config.max_epochs,\n",
    "        activation=config.activation,\n",
    "        initializer=config.initializer,\n",
    "        loss=config.loss,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    training_loss, train_accuracy, val_accuracy, Y_pred_train = FFNN.optimizer(\n",
    "        FFNN.max_epochs, FFNN.N_train, FFNN.batch_size, FFNN.alpha\n",
    "    )\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    # Make predictions on test set\n",
    "    Y_pred_test = FFNN.predict(FFNN.X_test, FFNN.N_test)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    train_acc, Y_true_train, Y_pred_train = accuracy(FFNN.Y_train, Y_pred_train, FFNN.N_train)\n",
    "    test_acc, Y_true_test, Y_pred_test = accuracy(FFNN.Y_test, Y_pred_test, FFNN.N_test)\n",
    "\n",
    "    return (train_acc, Y_true_train, Y_pred_train), (test_acc, Y_true_test, Y_pred_test)\n",
    "\n",
    "\n",
    "# Run training and testing\n",
    "Results = {}\n",
    "Results[\"train_pred_best\"], Results[\"test_pred_best\"] = train_and_test()\n",
    "\n",
    "# Logging best test accuracy as a bar chart\n",
    "wandb.init(project=PROJECT_NAME_KEY, entity=PROJECT_ENTITY, config=best_config)\n",
    "\n",
    "wandb.run.name = (\n",
    "    f\"confusion_matrix_{wandb.config.num_hidden_layers}_hn_{wandb.config.num_hidden_neurons}\"\n",
    "    f\"_opt_{wandb.config.optimizer}_act_{wandb.config.activation}\"\n",
    "    f\"_lr_{wandb.config.learning_rate}_bs_{wandb.config.batch_size}\"\n",
    "    f\"_init_{wandb.config.initializer}_ep_{wandb.config.max_epochs}\"\n",
    "    f\"_l2_{wandb.config.weight_decay}\"\n",
    ")\n",
    "\n",
    "# Log test accuracy as a bar chart\n",
    "data = [[\"test_pred_best\", Results[\"test_pred_best\"][0]]]\n",
    "table = wandb.Table(data=data, columns=[\"Configuration\", \"Test accuracy\"])\n",
    "wandb.log({\"Test Accuracy Bar Chart\": wandb.plot.bar(table, \"Configuration\", \"Test accuracy\", title=\"Test Accuracy for Best Model\")})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Log confusion matrix\n",
    "wandb.init(project=PROJECT_NAME_KEY, entity=PROJECT_ENTITY)\n",
    "wandb.sklearn.plot_confusion_matrix(\n",
    "    Results[\"train_pred_best\"][1], Results[\"train_pred_best\"][2], labels=list(range(10))\n",
    ")\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELECzZDdMXOx",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Question - 8\n",
    "Calling our model with MSE and CROSS with best params and check the relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxL3rSNOL4pD"
   },
   "outputs": [],
   "source": [
    "def train_model(loss_function, run_name_prefix):\n",
    "    \"\"\"Trains the FeedForwardNeuralNetwork with a specified loss function.\"\"\"\n",
    "    config = {\n",
    "        \"max_epochs\": 10,\n",
    "        \"num_hidden_layers\": 3,\n",
    "        \"num_hidden_neurons\": 128,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"optimizer\": NADAM_KEY,\n",
    "        \"batch_size\": 32,\n",
    "        \"activation\": RELU_KEY,\n",
    "        \"initializer\": XAVIER_KEY,\n",
    "        \"loss\": loss_function,\n",
    "    }\n",
    "\n",
    "    # Initialize WandB\n",
    "    wandb.init(project=PROJECT_NAME_KEY, entity=PROJECT_ENTITY, config=config)\n",
    "\n",
    "    # Set run name\n",
    "    wandb.run.name = (\n",
    "        f\"{run_name_prefix}_{wandb.config.num_hidden_layers}_hn_{wandb.config.num_hidden_neurons}\"\n",
    "        f\"_opt_{wandb.config.optimizer}_act_{wandb.config.activation}\"\n",
    "        f\"_lr_{wandb.config.learning_rate}_bs_{wandb.config.batch_size}\"\n",
    "        f\"_init_{wandb.config.initializer}_ep_{wandb.config.max_epochs}\"\n",
    "        f\"_l2_{wandb.config.weight_decay}\"\n",
    "    )\n",
    "\n",
    "    # Initialize neural network model\n",
    "    FFNN = FeedForwardNeuralNetwork(\n",
    "        num_hidden_layers=wandb.config.num_hidden_layers,\n",
    "        num_hidden_neurons=wandb.config.num_hidden_neurons,\n",
    "        X_train_raw=trainInFull,\n",
    "        Y_train_raw=trainOutFull,\n",
    "        N_train=N_train_full,\n",
    "        X_val_raw=validIn,\n",
    "        Y_val_raw=validOut,\n",
    "        N_val=N_validation,\n",
    "        X_test_raw=testIn,\n",
    "        Y_test_raw=testOut,\n",
    "        N_test=N_test,\n",
    "        optimizer=wandb.config.optimizer,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        weight_decay=wandb.config.weight_decay,\n",
    "        learning_rate=wandb.config.learning_rate,\n",
    "        max_epochs=wandb.config.max_epochs,\n",
    "        activation=wandb.config.activation,\n",
    "        initializer=wandb.config.initializer,\n",
    "        loss=wandb.config.loss,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    training_loss, training_accuracy, validation_accuracy, Y_pred_train = FFNN.optimizer(\n",
    "        FFNN.max_epochs, FFNN.N_train, FFNN.batch_size, FFNN.alpha\n",
    "    )\n",
    "\n",
    "    # Finish WandB run\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "# Train using Mean Squared Error\n",
    "train_model(MEAN_SQUARE_KEY, \"Mean_Square_Error\")\n",
    "\n",
    "# Train using Cross Entropy\n",
    "train_model(CROSS_ENTROPY_KEY, \"Cross_Entropy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5ZWrBuBOteN",
    "tags": []
   },
   "source": [
    "# Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLBHyI3YOnAn"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "(trainIn, trainOut), (testIn, testOut) = mnist.load_data()\n",
    "\n",
    "N_train_full = trainOut.shape[0]\n",
    "N_train = int(0.9*N_train_full)\n",
    "N_validation = int(0.1 * trainOut.shape[0])\n",
    "N_test = testOut.shape[0]\n",
    "\n",
    "\n",
    "idx  = np.random.choice(trainOut.shape[0], N_train_full, replace=False)\n",
    "idx2 = np.random.choice(testOut.shape[0], N_test, replace=False)\n",
    "\n",
    "trainInFull = trainIn[idx, :]\n",
    "trainOutFull = trainOut[idx]\n",
    "\n",
    "trainIn = trainInFull[:N_train,:]\n",
    "trainOut = trainOutFull[:N_train]\n",
    "\n",
    "validIn = trainInFull[N_train:, :]\n",
    "validOut = trainOutFull[N_train:]\n",
    "\n",
    "testIn = testIn[idx2, :]\n",
    "testOut = testOut[idx2]\n",
    "\n",
    "best_configs = dict(\n",
    "        max_epochs=10,\n",
    "        num_hidden_layers=3,\n",
    "        num_hidden_neurons=128,\n",
    "        weight_decay=0,\n",
    "        learning_rate=1e-3,\n",
    "        optimizer=NADAM_KEY,\n",
    "        batch_size=32,\n",
    "        activation=RELU_KEY,\n",
    "        initializer=XAVIER_KEY,\n",
    "        loss=CROSS_ENTROPY_KEY,\n",
    "    )\n",
    "\n",
    "wandb.init(project=PROJECT_NAME_KEY, entity=PROJECT_ENTITY, config = best_configs)\n",
    "\n",
    "wandb.run.name = \"MNIST_DATASET_1\" + str(wandb.config.num_hidden_layers) + \"_hn_\" + str(wandb.config.num_hidden_neurons) + \"_opt_\" + wandb.config.optimizer + \"_act_\" + wandb.config.activation + \"_lr_\" + str(wandb.config.learning_rate) + \"_bs_\"+str(wandb.config.batch_size) + \"_init_\" + wandb.config.initializer + \"_ep_\"+ str(wandb.config.max_epochs)+ \"_l2_\" + str(wandb.config.weight_decay)\n",
    "CONFIG = wandb.config\n",
    "\n",
    "FFNN = FeedForwardNeuralNetwork(\n",
    "    num_hidden_layers=CONFIG.num_hidden_layers,\n",
    "    num_hidden_neurons=CONFIG.num_hidden_neurons,\n",
    "    X_train_raw=trainInFull,\n",
    "    Y_train_raw=trainOutFull,\n",
    "    N_train = N_train_full,\n",
    "    X_val_raw = validIn,\n",
    "    Y_val_raw = validOut,\n",
    "    N_val = N_validation,\n",
    "    X_test_raw = testIn,\n",
    "    Y_test_raw = testOut,\n",
    "    N_test = N_test,\n",
    "    optimizer = CONFIG.optimizer,\n",
    "    batch_size = CONFIG.batch_size,\n",
    "    weight_decay = CONFIG.weight_decay,\n",
    "    learning_rate = CONFIG.learning_rate,\n",
    "    max_epochs = CONFIG.max_epochs,\n",
    "    activation = CONFIG.activation,\n",
    "    initializer = CONFIG.initializer,\n",
    "    loss = CONFIG.loss\n",
    "    )\n",
    "training_loss, trainingaccuracy, validationaccuracy, Y_pred_train = FFNN.optimizer(FFNN.max_epochs, FFNN.N_train, FFNN.batch_size, FFNN.alpha)\n",
    "wandb.finish()\n",
    "\n",
    "best_configs = dict(\n",
    "        max_epochs=10,\n",
    "        num_hidden_layers=5,\n",
    "        num_hidden_neurons=64,\n",
    "        weight_decay=0.0005,\n",
    "        learning_rate=1e-3,\n",
    "        optimizer=NADAM_KEY,\n",
    "        batch_size=32,\n",
    "        activation=RELU_KEY,\n",
    "        initializer=XAVIER_KEY,\n",
    "        loss=CROSS_ENTROPY_KEY,\n",
    "    )\n",
    "\n",
    "wandb.init(project=PROJECT_NAME_KEY, entity=PROJECT_ENTITY, config = best_configs)\n",
    "\n",
    "wandb.run.name = \"MNIST_DATASET_2\" + str(wandb.config.num_hidden_layers) + \"_hn_\" + str(wandb.config.num_hidden_neurons) + \"_opt_\" + wandb.config.optimizer + \"_act_\" + wandb.config.activation + \"_lr_\" + str(wandb.config.learning_rate) + \"_bs_\"+str(wandb.config.batch_size) + \"_init_\" + wandb.config.initializer + \"_ep_\"+ str(wandb.config.max_epochs)+ \"_l2_\" + str(wandb.config.weight_decay)\n",
    "CONFIG = wandb.config\n",
    "\n",
    "FFNN = FeedForwardNeuralNetwork(\n",
    "    num_hidden_layers=CONFIG.num_hidden_layers,\n",
    "    num_hidden_neurons=CONFIG.num_hidden_neurons,\n",
    "    X_train_raw=trainInFull,\n",
    "    Y_train_raw=trainOutFull,\n",
    "    N_train = N_train_full,\n",
    "    X_val_raw = validIn,\n",
    "    Y_val_raw = validOut,\n",
    "    N_val = N_validation,\n",
    "    X_test_raw = testIn,\n",
    "    Y_test_raw = testOut,\n",
    "    N_test = N_test,\n",
    "    optimizer = CONFIG.optimizer,\n",
    "    batch_size = CONFIG.batch_size,\n",
    "    weight_decay = CONFIG.weight_decay,\n",
    "    learning_rate = CONFIG.learning_rate,\n",
    "    max_epochs = CONFIG.max_epochs,\n",
    "    activation = CONFIG.activation,\n",
    "    initializer = CONFIG.initializer,\n",
    "    loss = CONFIG.loss\n",
    "    )\n",
    "training_loss, trainingaccuracy, validationaccuracy, Y_pred_train = FFNN.optimizer(FFNN.max_epochs, FFNN.N_train, FFNN.batch_size, FFNN.alpha)\n",
    "wandb.finish()\n",
    "\n",
    "best_configs = dict(\n",
    "        max_epochs=10,\n",
    "        num_hidden_layers=4,\n",
    "        num_hidden_neurons=128,\n",
    "        weight_decay=0,\n",
    "        learning_rate=1e-3,\n",
    "        optimizer=NADAM_KEY,\n",
    "        batch_size=32,\n",
    "        activation=RELU_KEY,\n",
    "        initializer=XAVIER_KEY,\n",
    "        loss=CROSS_ENTROPY_KEY,\n",
    "    )\n",
    "\n",
    "wandb.init(project=PROJECT_NAME_KEY, entity=PROJECT_ENTITY, config = best_configs)\n",
    "\n",
    "wandb.run.name = \"MNIST_DATASET_3\" + str(wandb.config.num_hidden_layers) + \"_hn_\" + str(wandb.config.num_hidden_neurons) + \"_opt_\" + wandb.config.optimizer + \"_act_\" + wandb.config.activation + \"_lr_\" + str(wandb.config.learning_rate) + \"_bs_\"+str(wandb.config.batch_size) + \"_init_\" + wandb.config.initializer + \"_ep_\"+ str(wandb.config.max_epochs)+ \"_l2_\" + str(wandb.config.weight_decay)\n",
    "CONFIG = wandb.config\n",
    "\n",
    "FFNN = FeedForwardNeuralNetwork(\n",
    "    num_hidden_layers=CONFIG.num_hidden_layers,\n",
    "    num_hidden_neurons=CONFIG.num_hidden_neurons,\n",
    "    X_train_raw=trainInFull,\n",
    "    Y_train_raw=trainOutFull,\n",
    "    N_train = N_train_full,\n",
    "    X_val_raw = validIn,\n",
    "    Y_val_raw = validOut,\n",
    "    N_val = N_validation,\n",
    "    X_test_raw = testIn,\n",
    "    Y_test_raw = testOut,\n",
    "    N_test = N_test,\n",
    "    optimizer = CONFIG.optimizer,\n",
    "    batch_size = CONFIG.batch_size,\n",
    "    weight_decay = CONFIG.weight_decay,\n",
    "    learning_rate = CONFIG.learning_rate,\n",
    "    max_epochs = CONFIG.max_epochs,\n",
    "    activation = CONFIG.activation,\n",
    "    initializer = CONFIG.initializer,\n",
    "    loss = CONFIG.loss\n",
    "    )\n",
    "training_loss, trainingaccuracy, validationaccuracy, Y_pred_train = FFNN.optimizer(FFNN.max_epochs, FFNN.N_train, FFNN.batch_size, FFNN.alpha)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MzyhH7e-wW06",
    "r1jrcToSKIPY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07e4163d41bd4aaebae2a76ccbb767a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a093c18d82f43af94229922b645532f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a2ef16481e94ca3bf877b27988b13c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1aa7b4c7f2044519b8f7e3d4bfbcb2ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c8e87e890224abca869161f20ff75e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1eec5dbf0fd04358b9073dce5268ca27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b0d2e5b583014d94895475b9e64a54fe",
       "IPY_MODEL_58894764e2354d73a77542b08f384940"
      ],
      "layout": "IPY_MODEL_1c8e87e890224abca869161f20ff75e3"
     }
    },
    "23a94a82a6ca42ec946fcf3907061507": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4ea330b8112144aaa609f227c69ab43d",
       "IPY_MODEL_8203cd648034444392447138fd311a3b"
      ],
      "layout": "IPY_MODEL_5eafd61b8548431a97329158b4e5e769"
     }
    },
    "258c07dded5b481587d4de2fa71b6064": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26ebecd763da4b8baddb829a24af7fe9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac505941d2844e47adcbee09a8f44f81",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_716e275fa6a94454b70212d91e93ddde",
      "value": 1
     }
    },
    "2d0df6063f8f4838bea906dc8ef9bfd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b59ac281ebf8412da8608ad546cd2371",
      "placeholder": "",
      "style": "IPY_MODEL_1a2ef16481e94ca3bf877b27988b13c2",
      "value": "0.014 MB of 0.014 MB uploaded\r"
     }
    },
    "436d84ba036341b5b43468382725e5c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4ea330b8112144aaa609f227c69ab43d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ccc92268f6e4ed68b4e4d04209d61e3",
      "placeholder": "",
      "style": "IPY_MODEL_df3cd51143304151bcf7424531ceb7e0",
      "value": "0.016 MB of 0.016 MB uploaded\r"
     }
    },
    "4fddc31e9c784ae9a6cf76277f004f27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7bc9827bf21c4cebb76408db886c397c",
       "IPY_MODEL_cdd58212b57d4a789b5cf6ff3d4edbf7"
      ],
      "layout": "IPY_MODEL_74aa8445b46e4002821ce45d3a31956b"
     }
    },
    "58894764e2354d73a77542b08f384940": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_258c07dded5b481587d4de2fa71b6064",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b16efa46a2c64d4cb164458b05ef7df4",
      "value": 1
     }
    },
    "5eafd61b8548431a97329158b4e5e769": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6187f791e4b84b8396c130b752ab73ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2d0df6063f8f4838bea906dc8ef9bfd6",
       "IPY_MODEL_26ebecd763da4b8baddb829a24af7fe9"
      ],
      "layout": "IPY_MODEL_1aa7b4c7f2044519b8f7e3d4bfbcb2ad"
     }
    },
    "6353176c740345eba3b2b15badc59fba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63caec8292094fe19acf9b6dcde0a5ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee57ab04d4d34ed987c2577756ec8bbe",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a9d8af854d34e3486727e81a701da30",
      "value": 1
     }
    },
    "716e275fa6a94454b70212d91e93ddde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "74aa8445b46e4002821ce45d3a31956b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bc9827bf21c4cebb76408db886c397c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d70e938fc75149de8717ad126b261fd4",
      "placeholder": "",
      "style": "IPY_MODEL_9d5a1f51bd1b4c8a88be3f606510490e",
      "value": "Waiting for wandb.init()...\r"
     }
    },
    "7ccc92268f6e4ed68b4e4d04209d61e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8203cd648034444392447138fd311a3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6353176c740345eba3b2b15badc59fba",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_436d84ba036341b5b43468382725e5c4",
      "value": 1
     }
    },
    "9a9d8af854d34e3486727e81a701da30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9d2705f0916c491d8bfefbb2c925681d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d5a1f51bd1b4c8a88be3f606510490e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9dc3b7c5a2e64d399e582f51d1c565dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1e8719afeec43cda8540476a5cbdffe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0a71a3f9fc848319aae3cbf73de0d22",
      "placeholder": "",
      "style": "IPY_MODEL_9dc3b7c5a2e64d399e582f51d1c565dd",
      "value": "0.012 MB of 0.012 MB uploaded\r"
     }
    },
    "ac505941d2844e47adcbee09a8f44f81": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0d2e5b583014d94895475b9e64a54fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d2705f0916c491d8bfefbb2c925681d",
      "placeholder": "",
      "style": "IPY_MODEL_c0a7112a248041a4b2fba524ad2aa38f",
      "value": "0.013 MB of 0.013 MB uploaded\r"
     }
    },
    "b16efa46a2c64d4cb164458b05ef7df4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b59ac281ebf8412da8608ad546cd2371": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0a7112a248041a4b2fba524ad2aa38f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cdd58212b57d4a789b5cf6ff3d4edbf7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a093c18d82f43af94229922b645532f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f0f35a8e969643b18eb62680fd9ba6c7",
      "value": 1
     }
    },
    "d70e938fc75149de8717ad126b261fd4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df3cd51143304151bcf7424531ceb7e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0a71a3f9fc848319aae3cbf73de0d22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee57ab04d4d34ed987c2577756ec8bbe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0f35a8e969643b18eb62680fd9ba6c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ff0e6f7f547e457992c173536cd1a7eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a1e8719afeec43cda8540476a5cbdffe",
       "IPY_MODEL_63caec8292094fe19acf9b6dcde0a5ed"
      ],
      "layout": "IPY_MODEL_07e4163d41bd4aaebae2a76ccbb767a7"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
